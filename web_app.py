import os
import sys
import time
import tempfile
import torch
import cv2
import numpy as np
from PIL import Image
import gradio as gr
from torchvision.io import read_video, write_video
from torchvision import transforms
import torch.nn.functional as F
from tqdm import tqdm
import traceback

# å¯¼å…¥FLAVRç›¸å…³æ¨¡å—
from model.FLAVR_arch import UNet_3D_3D
from dataset.transforms import ToTensorVideo, Resize

# æ¨¡å‹é…ç½®
MODEL_CONFIGS = {
    "flavr_2x": {
        "path": "models/flavr_2x.pth",
        "factor": 2,
        "description": "2xæ’å€¼ (30FPS â†’ 60FPS)"
    },
    "flavr_4x": {
        "path": "models/flavr_4x.pth", 
        "factor": 4,
        "description": "4xæ’å€¼ (30FPS â†’ 120FPS)"
    },
    "flavr_8x": {
        "path": "models/flavr_8x.pth",
        "factor": 8, 
        "description": "8xæ’å€¼ (30FPS â†’ 240FPS)"
    }
}



class FLAVRInterpolator:
    def __init__(self):
        self.models = {}  # ç¼“å­˜æ‰€æœ‰åŠ è½½çš„æ¨¡å‹
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.current_model_name = None
        

        
    def load_model(self, model_name):
        """åŠ è½½FLAVRæ¨¡å‹"""
        try:
            print(f"ğŸ”„ æ­£åœ¨åŠ è½½æ¨¡å‹: {model_name}")
            if model_name not in MODEL_CONFIGS:
                return False, f"æœªçŸ¥æ¨¡å‹: {model_name}"
            
            # æ£€æŸ¥æ˜¯å¦å·²ç»åŠ è½½äº†ç›¸åŒçš„æ¨¡å‹
            if model_name in self.models:
                self.current_model_name = model_name
                config = MODEL_CONFIGS[model_name]
                return True, f"æ¨¡å‹å·²åŠ è½½! {config['description']} (è®¾å¤‡: {self.device})"
            
            config = MODEL_CONFIGS[model_name]
            model_path = config["path"]
            
            if not os.path.exists(model_path):
                return False, f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}\nè¯·ä¸‹è½½æ¨¡å‹æ–‡ä»¶åˆ° {model_path}"
            
            # æ¨¡å‹å‚æ•°
            model_arch = "unet_18"
            nbr_frame = 4
            joinType = "concat"
            n_outputs = config["factor"] - 1
            
            # åˆ›å»ºæ¨¡å‹
            model = UNet_3D_3D(
                model_arch.lower(), 
                n_inputs=nbr_frame, 
                n_outputs=n_outputs, 
                joinType=joinType, 
                upmode="transpose"
            )
            # åŠ è½½æƒé‡
            checkpoint = torch.load(model_path, map_location=self.device)
            saved_state_dict = checkpoint['state_dict']
            saved_state_dict = {k.partition("module.")[-1]: v for k, v in saved_state_dict.items()}
            model.load_state_dict(saved_state_dict)
            
            model = model.to(self.device)
            model.eval()

            print("æ¨¡å‹åŠ è½½æˆåŠŸ")
            
            # ç¼“å­˜æ¨¡å‹
            self.models[model_name] = model
            self.current_model_name = model_name
            
            return True, f"æ¨¡å‹åŠ è½½æˆåŠŸ! {config['description']} (è®¾å¤‡: {self.device})"
            
        except Exception as e:
            return False, f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}"
    
    def get_model_factor(self):
        """è·å–å½“å‰æ¨¡å‹çš„æ’å€¼å€æ•°"""
        if self.current_model_name and self.current_model_name in MODEL_CONFIGS:
            return MODEL_CONFIGS[self.current_model_name]["factor"]
        return 2  # é»˜è®¤2x
    
    def get_loaded_models_info(self):
        """è·å–å·²åŠ è½½æ¨¡å‹çš„ä¿¡æ¯"""
        info = []
        for model_name in self.models.keys():
            if model_name in MODEL_CONFIGS:
                config = MODEL_CONFIGS[model_name]
                info.append(f"âœ… {config['description']} (å·²åŠ è½½)")
        return info
    
    def clear_models(self):
        """æ¸…ç†æ‰€æœ‰æ¨¡å‹ç¼“å­˜"""
        for model_name in list(self.models.keys()):
            if model_name != self.current_model_name:  # ä¿ç•™å½“å‰ä½¿ç”¨çš„æ¨¡å‹
                del self.models[model_name]
                print(f"ğŸ—‘ï¸  å·²é‡Šæ”¾æ¨¡å‹: {model_name}")
        
        torch.cuda.empty_cache()
    
    def switch_model(self, new_model_name):
        """åˆ‡æ¢æ¨¡å‹æ—¶é‡Šæ”¾å…¶ä»–æ¨¡å‹"""
        if new_model_name == self.current_model_name and new_model_name in self.models:
            return True, "æ¨¡å‹æœªå˜åŒ–"
        
        # é‡Šæ”¾é™¤æ–°æ¨¡å‹å¤–çš„æ‰€æœ‰å…¶ä»–æ¨¡å‹
        models_to_remove = []
        for model_name in self.models.keys():
            if model_name != new_model_name:
                models_to_remove.append(model_name)
        
        for model_name in models_to_remove:
            del self.models[model_name]
            print(f"ğŸ—‘ï¸  åˆ‡æ¢æ¨¡å‹æ—¶é‡Šæ”¾: {model_name}")
        
        # å¦‚æœæ–°æ¨¡å‹æœªåŠ è½½ï¼Œåˆ™åŠ è½½å®ƒ
        if new_model_name not in self.models:
            success, msg = self.load_model(new_model_name)
            if not success:
                return False, msg
        
        self.current_model_name = new_model_name
        return True, f"å·²åˆ‡æ¢åˆ° {new_model_name}"
    
    def get_memory_usage(self):
        """è·å–å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        if torch.cuda.is_available():
            return f"GPUå†…å­˜: {torch.cuda.memory_allocated() / 1024**3:.2f}GB / {torch.cuda.memory_reserved() / 1024**3:.2f}GB"
        else:
            return "CPUæ¨¡å¼"
    
    def video_to_tensor(self, video_path):
        """å°†è§†é¢‘è½¬æ¢ä¸ºå¼ é‡"""
        try:
            video_tensor, _, metadata = read_video(video_path)
            fps = metadata["video_fps"]
            duration = metadata.get("duration", 0)
            
            # è®¡ç®—è§†é¢‘é•¿åº¦ï¼ˆç§’ï¼‰
            if duration > 0:
                video_length = duration
            else:
                video_length = len(video_tensor) / fps if fps > 0 else 0
            
            print(f"è§†é¢‘é•¿åº¦: {video_length:.2f}ç§’")
            print(f"è§†é¢‘å¸§æ•°: {len(video_tensor)}")
            print(f"è§†é¢‘FPS: {fps}")
            
            return video_tensor, fps
        except Exception as e:
            raise Exception(f"è§†é¢‘è¯»å–å¤±è´¥: {str(e)}")
    
    def video_transform(self, video_tensor, target_size=None):
        """è§†é¢‘é¢„å¤„ç†"""
        T, H, W = video_tensor.size(0), video_tensor.size(1), video_tensor.size(2)
        
        if target_size is None:
            # è‡ªåŠ¨è°ƒæ•´åˆ°8çš„å€æ•°
            new_h = 8 * (H // 8)
            new_w = 8 * (W // 8)
        else:
            new_h, new_w = target_size
        
        print(f"åŸå§‹å°ºå¯¸: {H}x{W}, è°ƒæ•´åå°ºå¯¸: {new_h}x{new_w}")
        
        # æ£€æŸ¥å†…å­˜éœ€æ±‚å¹¶ç»™å‡ºè­¦å‘Š
        estimated_memory_gb = T * new_h * new_w * 3 * 4 / (1024**3)  # è½¬æ¢ä¸ºGB
        print(f"é¢„ä¼°å†…å­˜éœ€æ±‚: {estimated_memory_gb:.2f} GB")
        
        if estimated_memory_gb > 4:  # è¶…è¿‡4GBç»™å‡ºè­¦å‘Š
            print(f"âš ï¸  è­¦å‘Š: é¢„ä¼°å†…å­˜éœ€æ±‚ {estimated_memory_gb:.2f} GB è¾ƒé«˜ï¼Œå¯èƒ½å¯¼è‡´å†…å­˜ä¸è¶³")
        
        transform = transforms.Compose([
            ToTensorVideo(),
            Resize((new_h, new_w))
        ])
        
        try:
            video_tensor = transform(video_tensor)
        except RuntimeError as e:
            if "not enough memory" in str(e):
                # è®¡ç®—å»ºè®®çš„åˆ†è¾¨ç‡
                available_memory_gb = 2  # å‡è®¾å¯ç”¨å†…å­˜2GB
                scale_factor = (available_memory_gb / estimated_memory_gb) ** 0.5
                suggested_h = int(H * scale_factor)
                suggested_w = int(W * scale_factor)
                suggested_h = 8 * (suggested_h // 8)
                suggested_w = 8 * (suggested_w // 8)
                
                error_msg = f"""
âŒ å†…å­˜ä¸è¶³é”™è¯¯!

å½“å‰è§†é¢‘ä¿¡æ¯:
- åŸå§‹å°ºå¯¸: {H}x{W}
- ç›®æ ‡å°ºå¯¸: {new_h}x{new_w}
- å¸§æ•°: {T}
- é¢„ä¼°å†…å­˜éœ€æ±‚: {estimated_memory_gb:.2f} GB

ğŸ’¡ è§£å†³æ–¹æ¡ˆ:
1. ä½¿ç”¨æ›´å°çš„è§†é¢‘æ–‡ä»¶
2. æ‰‹åŠ¨è®¾ç½®è¾ƒå°çš„ç›®æ ‡å°ºå¯¸ (å»ºè®®: {suggested_h}x{suggested_w})
3. è£å‰ªè§†é¢‘é•¿åº¦
4. å¢åŠ ç³»ç»Ÿå†…å­˜

è¯·é‡æ–°ä¸Šä¼ è¾ƒå°çš„è§†é¢‘æˆ–æ‰‹åŠ¨è®¾ç½®ç›®æ ‡å°ºå¯¸ã€‚
                """
                raise Exception(error_msg)
            else:
                raise e
        
        return video_tensor, (new_h, new_w)
    
    def make_image(self, img):
        """å°†å¼ é‡è½¬æ¢ä¸ºå›¾åƒ"""
        q_im = img.data.mul(255.).clamp(0, 255).round()
        im = q_im.permute(1, 2, 0).cpu().numpy().astype(np.uint8)
        im = cv2.cvtColor(im, cv2.COLOR_RGB2BGR)
        return im
    
    def interpolate_video(self, video_path):
        """è§†é¢‘æ’å€¼ä¸»å‡½æ•°"""
        if not self.current_model_name or self.current_model_name not in self.models:
            raise Exception("è¯·å…ˆåŠ è½½æ¨¡å‹!")
        
        factor = self.get_model_factor()
        
        # è¯»å–è§†é¢‘
        video_tensor, original_fps = self.video_to_tensor(video_path)
        print(f"åŸå§‹è§†é¢‘FPS: {original_fps}")
        print(f"è§†é¢‘å¸§æ•°: {len(video_tensor)}")
        
        # æ£€æŸ¥è§†é¢‘å¸§æ•°æ˜¯å¦è¶³å¤Ÿ
        if len(video_tensor) < 4:
            raise Exception(f"è§†é¢‘å¸§æ•°ä¸è¶³! éœ€è¦è‡³å°‘4å¸§ï¼Œå½“å‰åªæœ‰{len(video_tensor)}å¸§")
        
        # é¢„å¤„ç†
        video_tensor, (new_h, new_w) = self.video_transform(video_tensor)
        print(f"è°ƒæ•´åå°ºå¯¸: {new_h}x{new_w}")
        print(f"è§†é¢‘å¼ é‡å½¢çŠ¶: {video_tensor.shape}")
        
        # æ£€æŸ¥å¼ é‡ç»´åº¦
        if len(video_tensor.shape) != 4:
            raise Exception(f"è§†é¢‘å¼ é‡ç»´åº¦é”™è¯¯! æœŸæœ›4ç»´ï¼Œå®é™…{len(video_tensor.shape)}ç»´")
        
        # å‡†å¤‡å¸§ç´¢å¼•
        nbr_frame = 4
        n_outputs = factor - 1
        
        # è·å–æ—¶é—´ç»´åº¦ï¼ˆç»è¿‡ToTensorVideoåï¼Œæ—¶é—´ç»´åº¦åœ¨ç¬¬1ç»´ï¼‰
        time_dim = video_tensor.size(1)
        
        # ç¡®ä¿æœ‰è¶³å¤Ÿçš„å¸§è¿›è¡Œæ’å€¼
        if time_dim < nbr_frame:
            raise Exception(f"è§†é¢‘å¤ªçŸ­ï¼Œæ— æ³•è¿›è¡Œæ’å€¼! éœ€è¦è‡³å°‘{nbr_frame}å¸§")
        
        # è®¡ç®—å¯ä»¥ç”Ÿæˆçš„æ’å€¼ç‰‡æ®µæ•°é‡
        num_segments = time_dim - nbr_frame + 1
        print(f"å¯ç”Ÿæˆæ’å€¼ç‰‡æ®µ: {num_segments}")
        
        if num_segments <= 0:
            raise Exception(f"è§†é¢‘å¤ªçŸ­ï¼Œæ— æ³•è¿›è¡Œæ’å€¼! éœ€è¦è‡³å°‘{nbr_frame}å¸§")
        
        idxs = torch.arange(time_dim).view(1, -1).unfold(1, size=nbr_frame, step=1).squeeze(0)
        
        # å°†è§†é¢‘å¼ é‡æŒ‰æ—¶é—´ç»´åº¦åˆ†å‰²æˆå¸§
        frames = [video_tensor[:, i, :, :] for i in range(time_dim)]
        outputs = []
        
        # æ·»åŠ ç¬¬ä¸€å¸§
        if len(idxs) > 0:
            outputs.append(frames[idxs[0][1]])
        else:
            raise Exception("æ— æ³•ç”Ÿæˆæ’å€¼ç´¢å¼•ï¼Œè§†é¢‘å¯èƒ½å¤ªçŸ­")
        
        # é€å¸§æ’å€¼
        current_model = self.models[self.current_model_name]
        for i in tqdm(range(len(idxs)), desc="æ’å€¼è¿›åº¦"):
            idx_set = idxs[i]
            inputs = [frames[idx_].to(self.device).unsqueeze(0) for idx_ in idx_set]
            
            with torch.no_grad():
                output_frames = current_model(inputs)
            
            output_frames = [of.squeeze(0).cpu().data for of in output_frames]
            outputs.extend(output_frames)
            outputs.append(frames[idx_set[2]].cpu().data)
        
        # è½¬æ¢ä¸ºå›¾åƒåˆ—è¡¨
        new_video = [self.make_image(im_) for im_ in outputs]
        
        # è®¡ç®—è¾“å‡ºå¸§ç‡ = åŸå§‹å¸§ç‡ * æ’å€¼å€æ•°
        output_fps = original_fps * factor
        print(f"æ’å€¼åè¾“å‡ºå¸§ç‡: {output_fps} FPS")
        
        return new_video, (new_w, new_h), output_fps
    
    def save_video(self, frames, size, fps, output_path):
        """ä¿å­˜è§†é¢‘"""
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(output_path, fourcc, fps, size)
        
        for frame in frames:
            out.write(frame)
        
        out.release()
        return output_path

# åˆ›å»ºå…¨å±€æ’å€¼å™¨å®ä¾‹
interpolator = FLAVRInterpolator()

def check_model_files():
    """æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
    status = {}
    for model_name, config in MODEL_CONFIGS.items():
        exists = os.path.exists(config["path"])
        status[model_name] = {
            "exists": exists,
            "path": config["path"],
            "description": config["description"]
        }
    return status

def process_video(video_file, model_name):
    """Gradioå¤„ç†å‡½æ•°"""
    if video_file is None:
        return None, "è¯·ä¸Šä¼ è§†é¢‘æ–‡ä»¶"
    
    if model_name is None:
        return None, "è¯·é€‰æ‹©æ¨¡å‹"
    
    try:
        # åˆ‡æ¢æ¨¡å‹ï¼ˆä¼šè‡ªåŠ¨é‡Šæ”¾å…¶ä»–æ¨¡å‹ï¼‰
        success, msg = interpolator.switch_model(model_name)
        if not success:
            return None, msg
        
        # è·å–è§†é¢‘æ–‡ä»¶è·¯å¾„
        video_path = video_file if isinstance(video_file, str) else video_file.name
        
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
        with tempfile.NamedTemporaryFile(suffix='.mp4', delete=False) as tmp_file:
            temp_output = tmp_file.name
        
        print("å¼€å§‹å¤„ç†è§†é¢‘")
        # å¤„ç†è§†é¢‘
        frames, size, fps = interpolator.interpolate_video(video_path)
        print("è§†é¢‘å¤„ç†å®Œæˆ")
        
        # ä¿å­˜è§†é¢‘
        output_path = interpolator.save_video(frames, size, fps, temp_output)
        
        factor = interpolator.get_model_factor()
        memory_usage = interpolator.get_memory_usage()
        return output_path, f"å¤„ç†å®Œæˆ! {factor}xæ’å€¼, è¾“å‡ºFPS: {fps}, å°ºå¯¸: {size[0]}x{size[1]} | {memory_usage}"
        
    except Exception as e:
        traceback.print_exc()
        error_msg = str(e)
        if "è§†é¢‘å¸§æ•°ä¸è¶³" in error_msg or "è§†é¢‘å¤ªçŸ­" in error_msg:
            return None, f"âŒ {error_msg}\nğŸ’¡ è¯·ä¸Šä¼ æ›´é•¿çš„è§†é¢‘æ–‡ä»¶ï¼ˆè‡³å°‘éœ€è¦4å¸§ï¼‰"
        elif "PyAV is not installed" in error_msg:
            return None, f"âŒ {error_msg}\nğŸ’¡ è¯·å®‰è£…PyAV: pip install av"
        else:
            return None, f"âŒ å¤„ç†å¤±è´¥: {error_msg}"

def clear_model_cache():
    """æ¸…ç†æ¨¡å‹ç¼“å­˜"""
    interpolator.clear_models()
    return interpolator.get_memory_usage(), "æ¨¡å‹ç¼“å­˜å·²æ¸…ç†"

def update_model_status():
    """æ›´æ–°æ¨¡å‹çŠ¶æ€æ˜¾ç¤º"""
    model_status = check_model_files()
    model_choices = []
    model_descriptions = []
    
    for model_name, config in MODEL_CONFIGS.items():
        status = model_status[model_name]
        if status["exists"]:
            model_choices.append(model_name)
            model_descriptions.append(f"âœ… {config['description']}")
        else:
            model_choices.append(model_name)
            model_descriptions.append(f"âŒ {config['description']} (æœªä¸‹è½½)")
    
    loaded_models_info = interpolator.get_loaded_models_info()
    status_text = "### æ¨¡å‹çŠ¶æ€:\n" + "\n".join([f"- {desc}" for desc in model_descriptions]) + \
                  "\n\n### å·²åŠ è½½æ¨¡å‹:\n" + "\n".join([f"- {info}" for info in loaded_models_info])
    
    return model_choices, status_text, interpolator.get_memory_usage()

def create_interface():
    """åˆ›å»ºGradioç•Œé¢"""
    with gr.Blocks(title="FLAVR è§†é¢‘è¡¥å¸§å·¥å…·", theme=gr.themes.Soft()) as demo:
        gr.Markdown("""
        # ğŸ¬ FLAVR è§†é¢‘è¡¥å¸§å·¥å…·
        
        FLAVRæ˜¯ä¸€ç§å¿«é€Ÿã€æ— æµçš„å¸§æ’å€¼æ–¹æ³•ï¼Œèƒ½å¤Ÿè¿›è¡Œå•æ¬¡å¤šå¸§é¢„æµ‹ã€‚ä¸Šä¼ æ‚¨çš„è§†é¢‘ï¼Œé€‰æ‹©æ’å€¼æ¨¡å‹ï¼Œå³å¯ç”Ÿæˆé«˜å¸§ç‡è§†é¢‘ã€‚
        
        ### ä½¿ç”¨è¯´æ˜:
        1. ä¸Šä¼ è§†é¢‘æ–‡ä»¶ (æ”¯æŒmp4, aviç­‰æ ¼å¼ï¼Œè‡³å°‘éœ€è¦4å¸§)
        2. é€‰æ‹©é¢„è®­ç»ƒæ¨¡å‹ (æ’å€¼å€æ•°è‡ªåŠ¨ç¡®å®š)
        3. ç‚¹å‡»å¼€å§‹å¤„ç†
        
        ### æ¨¡å‹è¯´æ˜:
        - **2xæ’å€¼**: é€‚ç”¨äº30FPSâ†’60FPS
        - **4xæ’å€¼**: é€‚ç”¨äº30FPSâ†’120FPS  
        - **8xæ’å€¼**: é€‚ç”¨äº30FPSâ†’240FPS
        
        ### ğŸš€ æ–°åŠŸèƒ½:
        - **è§†é¢‘é¢„è§ˆ**: æ”¯æŒè§†é¢‘ä¸Šä¼ å‰é¢„è§ˆ
        - **æ™ºèƒ½å†…å­˜ç®¡ç†**: æŒ‰éœ€åŠ è½½æ¨¡å‹ï¼Œåˆ‡æ¢æ—¶è‡ªåŠ¨é‡Šæ”¾å†…å­˜
        - **å®æ—¶çŠ¶æ€ç›‘æ§**: æ˜¾ç¤ºå†…å­˜ä½¿ç”¨å’Œæ¨¡å‹çŠ¶æ€
        
        ### âš ï¸ å†…å­˜ä½¿ç”¨æç¤º:
        - å¤„ç†å¤§åˆ†è¾¨ç‡è§†é¢‘æ—¶å¯èƒ½éœ€è¦å¤§é‡å†…å­˜
        - ç³»ç»Ÿä¼šè‡ªåŠ¨æ˜¾ç¤ºé¢„ä¼°å†…å­˜éœ€æ±‚
        - å¦‚é‡å†…å­˜ä¸è¶³ï¼Œè¯·ä½¿ç”¨è¾ƒå°çš„è§†é¢‘æ–‡ä»¶æˆ–é™ä½åˆ†è¾¨ç‡
        """)
        
        # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶çŠ¶æ€
        model_status = check_model_files()
        
        with gr.Row():
            with gr.Column(scale=1):
                # è¾“å…¥éƒ¨åˆ†
                video_input = gr.Video(
                    label="ä¸Šä¼ è§†é¢‘æ–‡ä»¶",
                    format="mp4"
                )
                
                # æ¨¡å‹é€‰æ‹©
                model_choices = []
                model_descriptions = []
                for model_name, config in MODEL_CONFIGS.items():
                    status = model_status[model_name]
                    if status["exists"]:
                        model_choices.append(model_name)
                        model_descriptions.append(f"âœ… {config['description']}")
                    else:
                        model_choices.append(model_name)
                        model_descriptions.append(f"âŒ {config['description']} (æœªä¸‹è½½)")
                
                model_input = gr.Dropdown(
                    choices=model_choices,
                    label="é€‰æ‹©æ¨¡å‹",
                    info="æ’å€¼å€æ•°ç”±æ¨¡å‹è‡ªåŠ¨ç¡®å®š",
                    value=model_choices[0] if model_choices else None
                )
                
                # æ˜¾ç¤ºæ¨¡å‹çŠ¶æ€
                model_status_text = gr.Markdown(
                    value="### æ¨¡å‹çŠ¶æ€:\n" + "\n".join([f"- {desc}" for desc in model_descriptions])
                )
                
                process_btn = gr.Button(
                    "å¼€å§‹å¤„ç†",
                    variant="primary",
                    size="lg"
                )
                
                # å†…å­˜ç®¡ç†
                memory_info = gr.Textbox(
                    label="å†…å­˜ä½¿ç”¨æƒ…å†µ",
                    value=interpolator.get_memory_usage(),
                    interactive=False
                )
                
                clear_btn = gr.Button(
                    "æ¸…ç†æ¨¡å‹ç¼“å­˜",
                    variant="secondary",
                    size="sm"
                )
                
                refresh_btn = gr.Button(
                    "åˆ·æ–°çŠ¶æ€",
                    variant="secondary",
                    size="sm"
                )
            
            with gr.Column(scale=1):
                # è¾“å‡ºéƒ¨åˆ†
                video_output = gr.Video(
                    label="å¤„ç†ç»“æœ",
                    format="mp4"
                )
                
                status_output = gr.Textbox(
                    label="å¤„ç†çŠ¶æ€",
                    interactive=False,
                    lines=3
                )
        
        # å¤„ç†é€»è¾‘
        process_btn.click(
            fn=process_video,
            inputs=[video_input, model_input],
            outputs=[video_output, status_output]
        )
        
        # æ¸…ç†ç¼“å­˜
        clear_btn.click(
            fn=clear_model_cache,
            inputs=[],
            outputs=[memory_info, status_output]
        )
        
        # åˆ·æ–°çŠ¶æ€
        refresh_btn.click(
            fn=update_model_status,
            inputs=[],
            outputs=[model_input, model_status_text, memory_info]
        )
        
        # æ·»åŠ æ¨¡å‹ä¸‹è½½è¯´æ˜
        gr.Markdown("""
        ### ğŸ“¥ æ¨¡å‹ä¸‹è½½è¯´æ˜:
        
        è¯·å°†ä¸‹è½½çš„æ¨¡å‹æ–‡ä»¶é‡å‘½åå¹¶æ”¾ç½®åˆ°ä»¥ä¸‹è·¯å¾„:
        
        ```
        models/
        â”œâ”€â”€ flavr_2x.pth  # 2xæ’å€¼æ¨¡å‹
        â”œâ”€â”€ flavr_4x.pth  # 4xæ’å€¼æ¨¡å‹
        â””â”€â”€ flavr_8x.pth  # 8xæ’å€¼æ¨¡å‹
        ```
        
        ### ğŸ”— æ¨¡å‹ä¸‹è½½é“¾æ¥:
        - [2xæ’å€¼æ¨¡å‹](https://drive.google.com/file/d/1IZe-39ZuXy3OheGJC-fT3shZocGYuNdH/view?usp=sharing) â†’ é‡å‘½åä¸º `flavr_2x.pth`
        - [4xæ’å€¼æ¨¡å‹](https://drive.google.com/file/d/1GARJK0Ti1gLH_O0spxAEqzbMwUKqE37S/view?usp=sharing) â†’ é‡å‘½åä¸º `flavr_4x.pth`
        - [8xæ’å€¼æ¨¡å‹](https://drive.google.com/file/d/1xoZqWJdIOjSaE2DtH4ifXKlRwFySm5Gq/view?usp=sharing) â†’ é‡å‘½åä¸º `flavr_8x.pth`
        
        ### âš¡ æ€§èƒ½ä¼˜åŒ–:
        - **æ™ºèƒ½åˆ‡æ¢**: åˆ‡æ¢æ¨¡å‹æ—¶è‡ªåŠ¨é‡Šæ”¾å…¶ä»–æ¨¡å‹ï¼ŒèŠ‚çœå†…å­˜
        - **å†…å­˜ç®¡ç†**: å¯æ‰‹åŠ¨æ¸…ç†ä¸éœ€è¦çš„æ¨¡å‹ç¼“å­˜
        - **GPUåŠ é€Ÿ**: æ”¯æŒCUDAåŠ é€Ÿï¼Œå¤§å¹…æå‡å¤„ç†é€Ÿåº¦
        - **å®æ—¶çŠ¶æ€**: æ˜¾ç¤ºå½“å‰å†…å­˜ä½¿ç”¨å’Œæ¨¡å‹åŠ è½½çŠ¶æ€
        
        ### æ³¨æ„äº‹é¡¹:
        - å¤„ç†æ—¶é—´å–å†³äºè§†é¢‘é•¿åº¦å’Œåˆ†è¾¨ç‡
        - å»ºè®®ä½¿ç”¨GPUåŠ é€Ÿå¤„ç†
        - è¾“å‡ºè§†é¢‘å°†è‡ªåŠ¨è°ƒæ•´åˆ°8çš„å€æ•°åˆ†è¾¨ç‡
        - æ”¯æŒå¸¸è§è§†é¢‘æ ¼å¼: MP4, AVI, MOVç­‰
        - åˆ‡æ¢æ¨¡å‹æ—¶ä¼šè‡ªåŠ¨é‡Šæ”¾å…¶ä»–æ¨¡å‹å†…å­˜ï¼Œæé«˜ç³»ç»Ÿæ€§èƒ½
        - **è§†é¢‘è¦æ±‚**: è‡³å°‘éœ€è¦4å¸§æ‰èƒ½è¿›è¡Œæ’å€¼å¤„ç†
        - **å¸§ç‡è¯´æ˜**: è¾“å‡ºå¸§ç‡ = åŸå§‹å¸§ç‡ Ã— æ’å€¼å€æ•° (å¦‚30FPSè¾“å…¥ï¼Œ2xæ’å€¼è¾“å‡º60FPS)
        - **å†…å­˜è¦æ±‚**: å¤§åˆ†è¾¨ç‡è§†é¢‘å¤„ç†éœ€è¦å……è¶³å†…å­˜ï¼Œç³»ç»Ÿä¼šæ˜¾ç¤ºé¢„ä¼°å†…å­˜éœ€æ±‚
        - **å†…å­˜ä¸è¶³æ—¶**: è¯·ä½¿ç”¨è¾ƒå°çš„è§†é¢‘æ–‡ä»¶æˆ–é™ä½è§†é¢‘åˆ†è¾¨ç‡
        """)
    
    return demo

if __name__ == "__main__":
    # åˆ›å»ºå¹¶å¯åŠ¨ç•Œé¢
    demo = create_interface()
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=True,
        debug=True
    ) 