import os
import sys
import time
import tempfile
import torch
import cv2
import numpy as np
from PIL import Image
import gradio as gr
from torchvision.io import read_video, write_video
from torchvision import transforms
import torch.nn.functional as F
from tqdm import tqdm
import traceback

# å¯¼å…¥FLAVRç›¸å…³æ¨¡å—
from model.FLAVR_arch import UNet_3D_3D
from dataset.transforms import ToTensorVideo, Resize

# æ¨¡å‹é…ç½®
MODEL_CONFIGS = {
    "flavr_2x": {
        "path": "models/flavr_2x.pth",
        "factor": 2,
        "description": "2xæ’å€¼ (30FPS â†’ 60FPS)"
    },
    "flavr_4x": {
        "path": "models/flavr_4x.pth", 
        "factor": 4,
        "description": "4xæ’å€¼ (30FPS â†’ 120FPS)"
    },
    "flavr_8x": {
        "path": "models/flavr_8x.pth",
        "factor": 8, 
        "description": "8xæ’å€¼ (30FPS â†’ 240FPS)"
    }
}



class FLAVRInterpolator:
    def __init__(self):
        self.models = {}  # ç¼“å­˜æ‰€æœ‰åŠ è½½çš„æ¨¡å‹
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.current_model_name = None
        

        
    def load_model(self, model_name):
        """åŠ è½½FLAVRæ¨¡å‹"""
        try:
            print(f"ğŸ”„ æ­£åœ¨åŠ è½½æ¨¡å‹: {model_name}")
            if model_name not in MODEL_CONFIGS:
                return False, f"æœªçŸ¥æ¨¡å‹: {model_name}"
            
            # æ£€æŸ¥æ˜¯å¦å·²ç»åŠ è½½äº†ç›¸åŒçš„æ¨¡å‹
            if model_name in self.models:
                self.current_model_name = model_name
                config = MODEL_CONFIGS[model_name]
                return True, f"æ¨¡å‹å·²åŠ è½½! {config['description']} (è®¾å¤‡: {self.device})"
            
            config = MODEL_CONFIGS[model_name]
            model_path = config["path"]
            
            if not os.path.exists(model_path):
                return False, f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}\nè¯·ä¸‹è½½æ¨¡å‹æ–‡ä»¶åˆ° {model_path}"
            
            # æ¨¡å‹å‚æ•°
            model_arch = "unet_18"
            nbr_frame = 4
            joinType = "concat"
            n_outputs = config["factor"] - 1
            
            # åˆ›å»ºæ¨¡å‹
            model = UNet_3D_3D(
                model_arch.lower(), 
                n_inputs=nbr_frame, 
                n_outputs=n_outputs, 
                joinType=joinType, 
                upmode="transpose"
            )
            # åŠ è½½æƒé‡
            checkpoint = torch.load(model_path, map_location=self.device)
            saved_state_dict = checkpoint['state_dict']
            saved_state_dict = {k.partition("module.")[-1]: v for k, v in saved_state_dict.items()}
            model.load_state_dict(saved_state_dict)
            
            model = model.to(self.device)
            model.eval()

            print("æ¨¡å‹åŠ è½½æˆåŠŸ")
            
            # ç¼“å­˜æ¨¡å‹
            self.models[model_name] = model
            self.current_model_name = model_name
            
            return True, f"æ¨¡å‹åŠ è½½æˆåŠŸ! {config['description']} (è®¾å¤‡: {self.device})"
            
        except Exception as e:
            return False, f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}"
    
    def get_model_factor(self):
        """è·å–å½“å‰æ¨¡å‹çš„æ’å€¼å€æ•°"""
        if self.current_model_name and self.current_model_name in MODEL_CONFIGS:
            return MODEL_CONFIGS[self.current_model_name]["factor"]
        return 2  # é»˜è®¤2x
    
    def get_loaded_models_info(self):
        """è·å–å·²åŠ è½½æ¨¡å‹çš„ä¿¡æ¯"""
        info = []
        for model_name in self.models.keys():
            if model_name in MODEL_CONFIGS:
                config = MODEL_CONFIGS[model_name]
                info.append(f"âœ… {config['description']} (å·²åŠ è½½)")
        return info
    
    def clear_models(self):
        """æ¸…ç†æ‰€æœ‰æ¨¡å‹ç¼“å­˜"""
        for model_name in list(self.models.keys()):
            if model_name != self.current_model_name:  # ä¿ç•™å½“å‰ä½¿ç”¨çš„æ¨¡å‹
                del self.models[model_name]
                print(f"ğŸ—‘ï¸  å·²é‡Šæ”¾æ¨¡å‹: {model_name}")
        
        torch.cuda.empty_cache()
    
    def switch_model(self, new_model_name):
        """åˆ‡æ¢æ¨¡å‹æ—¶é‡Šæ”¾å…¶ä»–æ¨¡å‹"""
        # ç¡®ä¿new_model_nameæ˜¯å­—ç¬¦ä¸²
        if not isinstance(new_model_name, str):
            return False, f"æ¨¡å‹åç§°å¿…é¡»æ˜¯å­—ç¬¦ä¸²ï¼Œå½“å‰ç±»å‹: {type(new_model_name)}"
        
        if new_model_name == self.current_model_name and new_model_name in self.models:
            return True, "æ¨¡å‹æœªå˜åŒ–"
        
        # é‡Šæ”¾é™¤æ–°æ¨¡å‹å¤–çš„æ‰€æœ‰å…¶ä»–æ¨¡å‹
        models_to_remove = []
        for model_name in list(self.models.keys()):  # ä½¿ç”¨list()é¿å…åœ¨è¿­ä»£æ—¶ä¿®æ”¹å­—å…¸
            if model_name != new_model_name:
                models_to_remove.append(model_name)
        
        for model_name in models_to_remove:
            del self.models[model_name]
            print(f"ğŸ—‘ï¸  åˆ‡æ¢æ¨¡å‹æ—¶é‡Šæ”¾: {model_name}")
        
        # å¦‚æœæ–°æ¨¡å‹æœªåŠ è½½ï¼Œåˆ™åŠ è½½å®ƒ
        if new_model_name not in self.models:
            success, msg = self.load_model(new_model_name)
            if not success:
                return False, msg
        
        self.current_model_name = new_model_name
        return True, f"å·²åˆ‡æ¢åˆ° {new_model_name}"
    
    def get_memory_usage(self):
        """è·å–å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        import psutil
        
        # è·å–ç³»ç»Ÿå†…å­˜ä½¿ç”¨
        memory = psutil.virtual_memory()
        system_memory = f"ç³»ç»Ÿå†…å­˜: {memory.used / 1024**3:.2f}GB / {memory.total / 1024**3:.2f}GB ({memory.percent}%)"
        
        if torch.cuda.is_available():
            gpu_memory = f"GPUå†…å­˜: {torch.cuda.memory_allocated() / 1024**3:.2f}GB / {torch.cuda.memory_reserved() / 1024**3:.2f}GB"
            return f"{system_memory} | {gpu_memory}"
        else:
            return f"{system_memory} | CPUæ¨¡å¼"
    
    def print_memory_status(self, stage=""):
        """æ‰“å°å†…å­˜çŠ¶æ€"""
        import psutil
        
        # ç³»ç»Ÿå†…å­˜
        memory = psutil.virtual_memory()
        print(f"{stage} ç³»ç»Ÿå†…å­˜: {memory.used / 1024**3:.2f}GB / {memory.total / 1024**3:.2f}GB ({memory.percent}%)")
        
        # GPUå†…å­˜
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated() / 1024**3
            reserved = torch.cuda.memory_reserved() / 1024**3
            print(f"{stage} GPUå†…å­˜: {allocated:.2f}GB / {reserved:.2f}GB")
        
        # Pythonè¿›ç¨‹å†…å­˜
        process = psutil.Process()
        process_memory = process.memory_info().rss / 1024**3
        print(f"{stage} è¿›ç¨‹å†…å­˜: {process_memory:.2f}GB")
    
    def video_to_tensor(self, video_path):
        """å°†è§†é¢‘è½¬æ¢ä¸ºå¼ é‡ - æ”¹è¿›ç‰ˆæœ¬"""
        try:
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(video_path):
                raise Exception(f"è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")
            
            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(video_path)
            if file_size == 0:
                raise Exception("è§†é¢‘æ–‡ä»¶ä¸ºç©º")
            
            print(f"è§†é¢‘æ–‡ä»¶å¤§å°: {file_size / (1024*1024):.2f} MB")
            
            # å°è¯•ä½¿ç”¨torchvisionè¯»å–
            try:
                video_tensor, _, metadata = read_video(video_path)
                fps = metadata["video_fps"]
                duration = metadata.get("duration", 0)
                
                # è®¡ç®—è§†é¢‘é•¿åº¦ï¼ˆç§’ï¼‰
                if duration > 0:
                    video_length = duration
                else:
                    video_length = len(video_tensor) / fps if fps > 0 else 0
                
                print(f"è§†é¢‘é•¿åº¦: {video_length:.2f}ç§’")
                print(f"è§†é¢‘å¸§æ•°: {len(video_tensor)}")
                print(f"è§†é¢‘FPS: {fps}")
                print(f"è§†é¢‘å°ºå¯¸: {video_tensor.shape}")
                
                return video_tensor, fps
                
            except Exception as e:
                print(f"torchvisionè¯»å–å¤±è´¥: {str(e)}")
                # å°è¯•ä½¿ç”¨OpenCVè¯»å–
                return self._video_to_tensor_opencv(video_path)
                
        except Exception as e:
            raise Exception(f"è§†é¢‘è¯»å–å¤±è´¥: {str(e)}")
    
    def _video_to_tensor_opencv(self, video_path):
        """ä½¿ç”¨OpenCVè¯»å–è§†é¢‘"""
        try:
            print("å°è¯•ä½¿ç”¨OpenCVè¯»å–è§†é¢‘...")
            cap = cv2.VideoCapture(video_path)
            
            if not cap.isOpened():
                raise Exception("æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶")
            
            # è·å–è§†é¢‘ä¿¡æ¯
            fps = cap.get(cv2.CAP_PROP_FPS)
            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            
            print(f"OpenCVè¯»å– - FPS: {fps}, å¸§æ•°: {frame_count}, å°ºå¯¸: {width}x{height}")
            
            # è¯»å–æ‰€æœ‰å¸§
            frames = []
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                frames.append(frame)
            
            cap.release()
            
            if len(frames) == 0:
                raise Exception("æ²¡æœ‰è¯»å–åˆ°ä»»ä½•å¸§")
            
            # è½¬æ¢ä¸ºtensoræ ¼å¼ (T, H, W, C)
            video_tensor = torch.stack([torch.from_numpy(frame) for frame in frames])
            
            print(f"OpenCVè¯»å–æˆåŠŸ - å¸§æ•°: {len(frames)}")
            return video_tensor, fps
            
        except Exception as e:
            raise Exception(f"OpenCVè¯»å–ä¹Ÿå¤±è´¥: {str(e)}")
    
    def _get_video_info(self, video_path):
        """è·å–è§†é¢‘åŸºæœ¬ä¿¡æ¯"""
        try:
            cap = cv2.VideoCapture(video_path)
            
            if not cap.isOpened():
                raise Exception("æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶")
            
            fps = cap.get(cv2.CAP_PROP_FPS)
            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            
            cap.release()
            
            return {
                'fps': fps,
                'frame_count': frame_count,
                'width': width,
                'height': height
            }
            
        except Exception as e:
            raise Exception(f"è·å–è§†é¢‘ä¿¡æ¯å¤±è´¥: {str(e)}")
    
    def _load_video_segment(self, video_path, start_frame, end_frame):
        """æµå¼åŠ è½½è§†é¢‘æ®µ"""
        try:
            cap = cv2.VideoCapture(video_path)
            
            if not cap.isOpened():
                raise Exception("æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶")
            
            # è®¾ç½®èµ·å§‹å¸§
            cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)
            
            # è¯»å–æŒ‡å®šèŒƒå›´çš„å¸§
            frames = []
            for i in range(start_frame, end_frame):
                ret, frame = cap.read()
                if not ret:
                    break
                frames.append(frame)
            
            cap.release()
            
            if len(frames) == 0:
                raise Exception(f"æ— æ³•è¯»å–å¸§ {start_frame}-{end_frame}")
            
            # è½¬æ¢ä¸ºtensoræ ¼å¼ (T, H, W, C)
            video_tensor = torch.stack([torch.from_numpy(frame) for frame in frames])
            
            # é¢„å¤„ç†å½“å‰æ®µ
            video_tensor, (new_h, new_w) = self.video_transform(video_tensor)
            
            print(f"æµå¼åŠ è½½æ®µ {start_frame}-{end_frame}: {len(frames)}å¸§, å°ºå¯¸: {new_h}x{new_w}")
            
            # æ¸…ç†framesåˆ—è¡¨ï¼Œé‡Šæ”¾å†…å­˜
            del frames
            
            return video_tensor, (new_w, new_h)
            
        except Exception as e:
            raise Exception(f"æµå¼åŠ è½½è§†é¢‘æ®µå¤±è´¥: {str(e)}")
    
    def video_transform(self, video_tensor, target_size=None):
        """è§†é¢‘é¢„å¤„ç†"""
        T, H, W = video_tensor.size(0), video_tensor.size(1), video_tensor.size(2)
        
        if target_size is None:
            # è‡ªåŠ¨è°ƒæ•´åˆ°8çš„å€æ•°
            new_h = 8 * (H // 8)
            new_w = 8 * (W // 8)
        else:
            new_h, new_w = target_size
        
        print(f"åŸå§‹å°ºå¯¸: {H}x{W}, è°ƒæ•´åå°ºå¯¸: {new_h}x{new_w}")
        
        # æ£€æŸ¥å†…å­˜éœ€æ±‚å¹¶ç»™å‡ºè­¦å‘Š
        estimated_memory_gb = T * new_h * new_w * 3 * 4 / (1024**3)  # è½¬æ¢ä¸ºGB
        print(f"é¢„ä¼°å†…å­˜éœ€æ±‚: {estimated_memory_gb:.2f} GB")
        
        if estimated_memory_gb > 4:  # è¶…è¿‡4GBç»™å‡ºè­¦å‘Š
            print(f"âš ï¸  è­¦å‘Š: é¢„ä¼°å†…å­˜éœ€æ±‚ {estimated_memory_gb:.2f} GB è¾ƒé«˜ï¼Œå¯èƒ½å¯¼è‡´å†…å­˜ä¸è¶³")
        
        transform = transforms.Compose([
            ToTensorVideo(),
            Resize((new_h, new_w))
        ])
        
        try:
            video_tensor = transform(video_tensor)
        except RuntimeError as e:
            if "not enough memory" in str(e):
                # è®¡ç®—å»ºè®®çš„åˆ†è¾¨ç‡
                available_memory_gb = 2  # å‡è®¾å¯ç”¨å†…å­˜2GB
                scale_factor = (available_memory_gb / estimated_memory_gb) ** 0.5
                suggested_h = int(H * scale_factor)
                suggested_w = int(W * scale_factor)
                suggested_h = 8 * (suggested_h // 8)
                suggested_w = 8 * (suggested_w // 8)
                
                error_msg = f"""
âŒ å†…å­˜ä¸è¶³é”™è¯¯!

å½“å‰è§†é¢‘ä¿¡æ¯:
- åŸå§‹å°ºå¯¸: {H}x{W}
- ç›®æ ‡å°ºå¯¸: {new_h}x{new_w}
- å¸§æ•°: {T}
- é¢„ä¼°å†…å­˜éœ€æ±‚: {estimated_memory_gb:.2f} GB

ğŸ’¡ è§£å†³æ–¹æ¡ˆ:
1. ä½¿ç”¨æ›´å°çš„è§†é¢‘æ–‡ä»¶
2. æ‰‹åŠ¨è®¾ç½®è¾ƒå°çš„ç›®æ ‡å°ºå¯¸ (å»ºè®®: {suggested_h}x{suggested_w})
3. è£å‰ªè§†é¢‘é•¿åº¦
4. å¢åŠ ç³»ç»Ÿå†…å­˜

è¯·é‡æ–°ä¸Šä¼ è¾ƒå°çš„è§†é¢‘æˆ–æ‰‹åŠ¨è®¾ç½®ç›®æ ‡å°ºå¯¸ã€‚
                """
                raise Exception(error_msg)
            else:
                raise e
        
        return video_tensor, (new_h, new_w)
    
    def make_image(self, img):
        """å°†å¼ é‡è½¬æ¢ä¸ºå›¾åƒ"""
        try:
            q_im = img.data.mul(255.).clamp(0, 255).round()
            im = q_im.permute(1, 2, 0).cpu().numpy().astype(np.uint8)
            
            # æ³¨æ„ï¼šOpenCVè¯»å–çš„æ˜¯BGRæ ¼å¼ï¼Œto_tensorä¿æŒBGRæ ¼å¼
            # æ‰€ä»¥è¿™é‡Œä¸éœ€è¦é¢œè‰²ç©ºé—´è½¬æ¢ï¼Œç›´æ¥è¿”å›BGRæ ¼å¼
            # im = cv2.cvtColor(im, cv2.COLOR_RGB2BGR)  # åˆ é™¤è¿™è¡Œï¼Œä¿æŒBGRæ ¼å¼
            
            # éªŒè¯å›¾åƒæ•°æ®
            if im is None or im.size == 0:
                print("è­¦å‘Š: ç”Ÿæˆçš„å›¾åƒä¸ºç©º")
                return None
            
            if np.any(np.isnan(im)) or np.any(np.isinf(im)):
                print("è­¦å‘Š: å›¾åƒåŒ…å«æ— æ•ˆæ•°æ®")
                return None
            
            return im
            
        except Exception as e:
            print(f"å›¾åƒè½¬æ¢å¤±è´¥: {str(e)}")
            return None
    
    def validate_frame(self, frame):
        """éªŒè¯å¸§æ•°æ®"""
        if frame is None:
            return False
        
        if not isinstance(frame, np.ndarray):
            return False
        
        if len(frame.shape) != 3:
            return False
        
        if frame.size == 0:
            return False
        
        if np.any(np.isnan(frame)) or np.any(np.isinf(frame)):
            return False
        
        # éªŒè¯é¢œè‰²é€šé“
        if frame.shape[2] != 3:
            print(f"è­¦å‘Š: å¸§é¢œè‰²é€šé“æ•°é”™è¯¯: {frame.shape[2]}")
            return False
        
        # éªŒè¯é¢œè‰²å€¼èŒƒå›´
        if frame.min() < 0 or frame.max() > 255:
            print(f"è­¦å‘Š: å¸§é¢œè‰²å€¼è¶…å‡ºèŒƒå›´: [{frame.min()}, {frame.max()}]")
            return False
        
        return True
    
    def interpolate_video(self, video_path, segment_duration=10):
        """è§†é¢‘æ’å€¼ä¸»å‡½æ•° - çœŸæ­£çš„åˆ†æ®µå­˜å‚¨å¤„ç†"""
        if not self.current_model_name or self.current_model_name not in self.models:
            raise Exception("è¯·å…ˆåŠ è½½æ¨¡å‹!")
        
        factor = self.get_model_factor()
        
        # è·å–è§†é¢‘åŸºæœ¬ä¿¡æ¯ï¼ˆä¸åŠ è½½å…¨éƒ¨æ•°æ®ï¼‰
        video_info = self._get_video_info(video_path)
        original_fps = video_info['fps']
        total_frames = video_info['frame_count']
        width = video_info['width']
        height = video_info['height']
        
        print(f"è§†é¢‘ä¿¡æ¯: {width}x{height}, {original_fps}FPS, {total_frames}å¸§")
        
        # æ£€æŸ¥è§†é¢‘å¸§æ•°æ˜¯å¦è¶³å¤Ÿ
        if total_frames < 4:
            raise Exception(f"è§†é¢‘å¸§æ•°ä¸è¶³! éœ€è¦è‡³å°‘4å¸§ï¼Œå½“å‰åªæœ‰{total_frames}å¸§")
        
        # ç®€åŒ–åˆ†æ®µç­–ç•¥ï¼šæ ¹æ®4çš„å€æ•°å’Œæ—¶é•¿åˆ†æ®µ
        min_frames_per_segment = 4  # FLAVRéœ€è¦è‡³å°‘4å¸§
        target_frames_per_segment = int(segment_duration * original_fps)
        
        # ç¡®ä¿æ¯æ®µå¸§æ•°æ˜¯4çš„å€æ•°
        frames_per_segment = 4 * (target_frames_per_segment // 4)  # å‘ä¸‹å–æ•´åˆ°4çš„å€æ•°
        if frames_per_segment < min_frames_per_segment:
            frames_per_segment = min_frames_per_segment
        
        print(f"åˆ†æ®µè®¡ç®—: ç›®æ ‡æ—¶é•¿{segment_duration}ç§’, è§†é¢‘{original_fps}FPS, æ€»å¸§æ•°{total_frames}")
        print(f"æ¯æ®µå¸§æ•°: {frames_per_segment}å¸§ (4çš„å€æ•°)")
        
        # è®¡ç®—åˆ†æ®µæ•°
        num_segments = (total_frames + frames_per_segment - 1) // frames_per_segment
        
        # æ£€æŸ¥æœ€åä¸€æ®µæ˜¯å¦éœ€è¦è¡¥å¸§
        last_segment_frames = total_frames - (num_segments - 1) * frames_per_segment
        if last_segment_frames > 0 and last_segment_frames < min_frames_per_segment:
            print(f"æœ€åä¸€æ®µå¸§æ•°ä¸è¶³({last_segment_frames}å¸§ < {min_frames_per_segment}å¸§)ï¼Œå°†è¡¥å¸§åˆ°{min_frames_per_segment}å¸§")
        
        print(f"åˆ†æ®µç­–ç•¥: {num_segments}æ®µï¼Œæ¯æ®µ{frames_per_segment}å¸§")
        
        # æ˜¾ç¤ºæ¯æ®µçš„å¸§æ•°åˆ†å¸ƒ
        for i in range(num_segments):
            start_frame = i * frames_per_segment
            end_frame = min(start_frame + frames_per_segment, total_frames)
            segment_frames = end_frame - start_frame
            print(f"  ç¬¬{i+1}æ®µ: å¸§{start_frame}-{end_frame-1} (å…±{segment_frames}å¸§)")
            
            if segment_frames < min_frames_per_segment and segment_frames > 0:
                print(f"    âš ï¸  æœ€åä¸€æ®µå¸§æ•°ä¸è¶³ï¼Œå°†ä½¿ç”¨å¤åˆ¶ç­–ç•¥è¡¥è¶³")
        
        print(f"åˆ†æ®µå­˜å‚¨å¤„ç†: {num_segments}æ®µï¼Œæ¯æ®µçº¦{segment_duration}ç§’ ({frames_per_segment}å¸§)")
        
        # æ˜¾ç¤ºåˆå§‹å†…å­˜çŠ¶æ€
        self.print_memory_status("åˆå§‹")
        
        # åˆ›å»ºä¸´æ—¶ç›®å½•å­˜å‚¨åˆ†æ®µè§†é¢‘
        temp_dir = tempfile.mkdtemp(prefix="flavr_segments_")
        segment_files = []
        
        try:
            # åˆ†æ®µå¤„ç†å¹¶ç«‹å³ä¿å­˜
            total_processed_frames = 0
            for segment_idx in tqdm(range(num_segments), desc="åˆ†æ®µå¤„ç†è¿›åº¦"):
                start_frame = segment_idx * frames_per_segment
                end_frame = min(start_frame + frames_per_segment, total_frames)
                segment_frame_count = end_frame - start_frame
                
                print(f"å¤„ç†ç¬¬ {segment_idx + 1}/{num_segments} æ®µ: å¸§ {start_frame}-{end_frame} (å…±{segment_frame_count}å¸§)")
                
                # æµå¼è¯»å–å½“å‰æ®µ
                segment_tensor, segment_size = self._load_video_segment(video_path, start_frame, end_frame)
                
                # æ£€æŸ¥æœ€åä¸€æ®µæ˜¯å¦éœ€è¦è¡¥å¸§
                if segment_idx == num_segments - 1 and segment_frame_count < 4:
                    print(f"ç¬¬{segment_idx + 1}æ®µå¸§æ•°ä¸è¶³({segment_frame_count}å¸§)ï¼Œè¡¥å¸§åˆ°4å¸§")
                    # å¤åˆ¶æœ€åä¸€å¸§æ¥è¡¥è¶³åˆ°4å¸§
                    last_frame = segment_tensor[:, -1:, :, :]  # å–æœ€åä¸€å¸§
                    while segment_tensor.size(1) < 4:
                        segment_tensor = torch.cat([segment_tensor, last_frame], dim=1)
                    print(f"è¡¥å¸§å: {segment_tensor.size(1)}å¸§")
                
                # å¤„ç†å½“å‰æ®µ
                segment_outputs = self._interpolate_segment(segment_tensor, factor, segment_idx, num_segments)
                
                if segment_outputs:
                    # ç«‹å³è½¬æ¢ä¸ºå›¾åƒå¹¶ä¿å­˜å½“å‰æ®µ
                    segment_images = []
                    for i, im_ in enumerate(segment_outputs):
                        frame = self.make_image(im_)
                        if self.validate_frame(frame):
                            segment_images.append(frame)
                        else:
                            print(f"è­¦å‘Š: ç¬¬{segment_idx + 1}æ®µç¬¬{i}å¸§æ— æ•ˆï¼Œè·³è¿‡")
                    
                    if segment_images:
                        # ä¿å­˜å½“å‰æ®µåˆ°ä¸´æ—¶æ–‡ä»¶
                        segment_file = os.path.join(temp_dir, f"segment_{segment_idx:03d}.mp4")
                        output_fps = original_fps * factor
                        
                        # ä¿å­˜å½“å‰æ®µè§†é¢‘
                        self._save_segment_video(segment_images, segment_size, output_fps, segment_file)
                        segment_files.append(segment_file)
                        
                        expected_output_frames = segment_frame_count * factor
                        print(f"ç¬¬{segment_idx + 1}æ®µä¿å­˜å®Œæˆ: è¾“å…¥{segment_frame_count}å¸§ -> è¾“å‡º{len(segment_images)}å¸§ (æœŸæœ›{expected_output_frames}å¸§)")
                        total_processed_frames += len(segment_images)
                    else:
                        print(f"è­¦å‘Š: ç¬¬{segment_idx + 1}æ®µæ²¡æœ‰æœ‰æ•ˆå¸§ï¼Œè·³è¿‡")
                else:
                    print(f"è­¦å‘Š: ç¬¬{segment_idx + 1}æ®µæ²¡æœ‰è¾“å‡ºï¼Œè·³è¿‡")
                
                # ç«‹å³æ¸…ç†å†…å­˜
                del segment_tensor, segment_outputs, segment_images
                
                # æ¸…ç†framesåˆ—è¡¨
                if 'frames' in locals():
                    del frames
                
                # æ¸…ç†GPUå†…å­˜
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()  # ç¡®ä¿GPUæ“ä½œå®Œæˆ
                
                # å¼ºåˆ¶åƒåœ¾å›æ”¶
                import gc
                gc.collect()
                
                print(f"ç¬¬{segment_idx + 1}æ®µå†…å­˜æ¸…ç†å®Œæˆ")
                
                # æ˜¾ç¤ºå½“å‰å†…å­˜ä½¿ç”¨æƒ…å†µ
                self.print_memory_status(f"ç¬¬{segment_idx + 1}æ®µå")
            
            print(f"åˆ†æ®µå¤„ç†å®Œæˆï¼Œç´¯è®¡è¾“å‡ºå¸§æ•°: {total_processed_frames}å¸§")
            
            if not segment_files:
                raise Exception(f"æ²¡æœ‰ç”Ÿæˆä»»ä½•è¾“å‡ºæ®µ! è¯·æ£€æŸ¥è§†é¢‘æ˜¯å¦æœ‰æ•ˆï¼Œæˆ–å°è¯•è°ƒæ•´åˆ†æ®µæ—¶é•¿å‚æ•°ã€‚")
            
            # åˆå¹¶æ‰€æœ‰åˆ†æ®µè§†é¢‘
            print("åˆå¹¶åˆ†æ®µè§†é¢‘...")
            final_video_path = self._merge_segment_videos(segment_files, original_fps * factor)
            
            # éªŒè¯æœ€ç»ˆè§†é¢‘çš„å¸§æ•°
            try:
                cap = cv2.VideoCapture(final_video_path)
                final_frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
                final_fps = cap.get(cv2.CAP_PROP_FPS)
                cap.release()
                
                expected_frames = total_frames * factor
                print(f"å¸§æ•°éªŒè¯: åŸè§†é¢‘{total_frames}å¸§ -> æœŸæœ›{expected_frames}å¸§ -> å®é™…{final_frame_count}å¸§")
                print(f"å¸§ç‡éªŒè¯: åŸè§†é¢‘{original_fps}FPS -> æœŸæœ›{original_fps * factor}FPS -> å®é™…{final_fps}FPS")
                
                # è¯¦ç»†åˆ†ææ¯æ®µçš„å¸§æ•°
                print("åˆ†æ®µå¸§æ•°åˆ†æ:")
                segment_total = 0
                for i, segment_file in enumerate(segment_files):
                    if os.path.exists(segment_file):
                        cap = cv2.VideoCapture(segment_file)
                        segment_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
                        cap.release()
                        segment_total += segment_frames
                        print(f"  ç¬¬{i+1}æ®µ: {segment_frames}å¸§")
                    else:
                        print(f"  ç¬¬{i+1}æ®µ: æ–‡ä»¶ä¸å­˜åœ¨")
                
                print(f"  åˆ†æ®µæ€»è®¡: {segment_total}å¸§")
                
                if final_frame_count != expected_frames:
                    print(f"âŒ é”™è¯¯: è¾“å‡ºå¸§æ•°({final_frame_count})ä¸æœŸæœ›å¸§æ•°({expected_frames})ä¸ç¬¦")
                    print(f"   å·®å¼‚: {final_frame_count - expected_frames}å¸§")
                    
                    # å¦‚æœå¸§æ•°ä¸å¯¹ï¼Œå°è¯•ä¿®å¤
                    if abs(final_frame_count - expected_frames) <= 5:  # å…è®¸5å¸§çš„è¯¯å·®
                        print("âš ï¸  å¸§æ•°å·®å¼‚è¾ƒå°ï¼Œå¯èƒ½æ˜¯åˆå¹¶è¿‡ç¨‹ä¸­çš„è¯¯å·®")
                    else:
                        print("âŒ  å¸§æ•°å·®å¼‚è¾ƒå¤§ï¼Œå¯èƒ½å­˜åœ¨å¤„ç†é”™è¯¯")
                else:
                    print(f"âœ… å¸§æ•°éªŒè¯é€šè¿‡: {final_frame_count}å¸§")
                    
            except Exception as e:
                print(f"å¸§æ•°éªŒè¯å¤±è´¥: {str(e)}")
            
            print(f"åˆ†æ®µå­˜å‚¨å¤„ç†å®Œæˆ: {num_segments}æ®µï¼Œæ¯æ®µçº¦{segment_duration}ç§’")
            
            return final_video_path, segment_size, original_fps * factor
            
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            for segment_file in segment_files:
                try:
                    if os.path.exists(segment_file):
                        os.remove(segment_file)
                except:
                    pass
            try:
                os.rmdir(temp_dir)
            except:
                pass
    
    def _interpolate_segment(self, segment_tensor, factor, segment_idx=0, total_segments=1):
        """å¤„ç†å•ä¸ªè§†é¢‘æ®µ - æŒ‰ç…§å®˜æ–¹demoé€»è¾‘"""
        nbr_frame = 4
        n_outputs = factor - 1
        
        # è·å–æ—¶é—´ç»´åº¦
        time_dim = segment_tensor.size(1)
        
        # å°†è§†é¢‘å¼ é‡æŒ‰æ—¶é—´ç»´åº¦åˆ†å‰²æˆå¸§
        frames = [segment_tensor[:, i, :, :] for i in range(time_dim)]
        
        # ç›®æ ‡è¾“å‡ºå¸§æ•°ï¼šè¾“å…¥å¸§æ•° * factor
        target_output_frames = time_dim * factor
        print(f"ç¬¬{segment_idx + 1}æ®µç›®æ ‡: è¾“å…¥{time_dim}å¸§ -> è¾“å‡º{target_output_frames}å¸§")
        
        # å¤„ç†ä¸è¶³4å¸§çš„æƒ…å†µ
        if time_dim < nbr_frame:
            print(f"ç¬¬{segment_idx + 1}æ®µå¸§æ•°ä¸è¶³({time_dim}å¸§)ï¼Œä½¿ç”¨å¤åˆ¶ç­–ç•¥")
            # ç®€å•å¤åˆ¶ç­–ç•¥
            outputs = []
            for frame in frames:
                outputs.extend([frame] * factor)  # æ¯å¸§å¤åˆ¶factoræ¬¡
            print(f"ç¬¬{segment_idx + 1}æ®µå¤åˆ¶å¤„ç†å®Œæˆ: è¾“å…¥{time_dim}å¸§ -> è¾“å‡º{len(outputs)}å¸§")
            return outputs
        
        # æ­£å¸¸å¤„ç†ï¼ˆ4å¸§æˆ–ä»¥ä¸Šï¼‰- æŒ‰ç…§å®˜æ–¹é€»è¾‘
        idxs = torch.arange(time_dim).view(1, -1).unfold(1, size=nbr_frame, step=1).squeeze(0)
        
        print(f"ç¬¬{segment_idx + 1}æ®µè°ƒè¯•: {time_dim}å¸§, {nbr_frame}å¸§çª—å£, {len(idxs)}ä¸ªç´¢å¼•ç»„")
        
        outputs = []  # å­˜å‚¨è¾“å…¥å’Œæ’å€¼å¸§
        current_model = self.models[self.current_model_name]
        
        # æŒ‰ç…§å®˜æ–¹é€»è¾‘ï¼šæ·»åŠ ç¬¬ä¸€å¸§ï¼ˆç¬¬2å¸§ï¼‰
        outputs.append(frames[idxs[0][1]])
        
        # å¤„ç†æ¯ä¸ª4å¸§çª—å£
        for i in range(len(idxs)):
            idx_set = idxs[i]
            inputs = [frames[idx_].to(self.device).unsqueeze(0) for idx_ in idx_set]
            
            with torch.no_grad():
                output_frames = current_model(inputs)
            
            output_frames = [of.squeeze(0).cpu().data for of in output_frames]
            outputs.extend(output_frames)  # æ·»åŠ æ’å€¼å¸§
            outputs.append(inputs[2].squeeze(0).cpu().data)  # æ·»åŠ ç¬¬3å¸§
            
            # æ¸…ç†GPUå†…å­˜
            del inputs, output_frames
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
        
        print(f"ç¬¬{segment_idx + 1}æ®µå¤„ç†å®Œæˆ: è¾“å…¥{time_dim}å¸§ -> è¾“å‡º{len(outputs)}å¸§")
        return outputs
    
    def save_video(self, frames, size, fps, output_path):
        """ä¿å­˜è§†é¢‘ - æ”¹è¿›ç‰ˆæœ¬"""
        try:
            # å°è¯•ä½¿ç”¨H.264ç¼–ç ï¼Œæ›´å…¼å®¹
            fourcc = cv2.VideoWriter_fourcc(*'H264')
            out = cv2.VideoWriter(output_path, fourcc, fps, size)
            
            if not out.isOpened():
                # å¦‚æœH.264ä¸å¯ç”¨ï¼Œå°è¯•mp4v
                print("H.264ç¼–ç ä¸å¯ç”¨ï¼Œå°è¯•mp4vç¼–ç ")
                fourcc = cv2.VideoWriter_fourcc(*'mp4v')
                out = cv2.VideoWriter(output_path, fourcc, fps, size)
                
                if not out.isOpened():
                    # å¦‚æœmp4vä¹Ÿä¸å¯ç”¨ï¼Œå°è¯•XVID
                    print("mp4vç¼–ç ä¸å¯ç”¨ï¼Œå°è¯•XVIDç¼–ç ")
                    output_path = output_path.replace('.mp4', '.avi')
                    fourcc = cv2.VideoWriter_fourcc(*'XVID')
                    out = cv2.VideoWriter(output_path, fourcc, fps, size)
            
            if not out.isOpened():
                raise Exception("æ— æ³•åˆ›å»ºè§†é¢‘å†™å…¥å™¨ï¼Œè¯·æ£€æŸ¥ç¼–ç å™¨æ”¯æŒ")
            
            print(f"ä½¿ç”¨ç¼–ç å™¨: {chr(fourcc & 0xFF) + chr((fourcc >> 8) & 0xFF) + chr((fourcc >> 16) & 0xFF) + chr((fourcc >> 24) & 0xFF)}")
            
            for frame in frames:
                out.write(frame)
            
            out.release()
            return output_path
            
        except Exception as e:
            print(f"è§†é¢‘ä¿å­˜å¤±è´¥: {str(e)}")
            # å°è¯•ä½¿ç”¨PILå’Œffmpegä¿å­˜
            return self._save_video_alternative(frames, size, fps, output_path)
    
    def _save_video_alternative(self, frames, size, fps, output_path):
        """å¤‡ç”¨è§†é¢‘ä¿å­˜æ–¹æ³•"""
        try:
            print("ä½¿ç”¨å¤‡ç”¨æ–¹æ³•ä¿å­˜è§†é¢‘...")
            
            # ä½¿ç”¨PILä¿å­˜å¸§ä¸ºä¸´æ—¶æ–‡ä»¶
            temp_dir = tempfile.mkdtemp()
            frame_files = []
            
            for i, frame in enumerate(frames):
                # è½¬æ¢BGRåˆ°RGB
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                pil_image = Image.fromarray(frame_rgb)
                
                frame_path = os.path.join(temp_dir, f"frame_{i:06d}.png")
                pil_image.save(frame_path)
                frame_files.append(frame_path)
            
            # ä½¿ç”¨ffmpegåˆæˆè§†é¢‘
            import subprocess
            
            # æ„å»ºffmpegå‘½ä»¤
            ffmpeg_cmd = [
                'ffmpeg', '-y',  # è¦†ç›–è¾“å‡ºæ–‡ä»¶
                '-framerate', str(fps),
                '-i', os.path.join(temp_dir, 'frame_%06d.png'),
                '-c:v', 'libx264',
                '-pix_fmt', 'yuv420p',
                '-crf', '23',
                output_path
            ]
            
            print(f"æ‰§è¡Œffmpegå‘½ä»¤: {' '.join(ffmpeg_cmd)}")
            
            result = subprocess.run(ffmpeg_cmd, capture_output=True, text=True)
            
            if result.returncode != 0:
                print(f"ffmpegé”™è¯¯: {result.stderr}")
                raise Exception(f"ffmpegæ‰§è¡Œå¤±è´¥: {result.stderr}")
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            for frame_file in frame_files:
                try:
                    os.remove(frame_file)
                except:
                    pass
            try:
                os.rmdir(temp_dir)
            except:
                pass
            
            return output_path
            
        except Exception as e:
            print(f"å¤‡ç”¨ä¿å­˜æ–¹æ³•ä¹Ÿå¤±è´¥: {str(e)}")
            raise Exception(f"è§†é¢‘ä¿å­˜å¤±è´¥: {str(e)}")
    
    def _save_segment_video(self, frames, size, fps, output_path):
        """ä¿å­˜åˆ†æ®µè§†é¢‘"""
        try:
            # ç›´æ¥ä½¿ç”¨ffmpegä¿å­˜ï¼Œé¿å…OpenCVçš„ç¼–ç é—®é¢˜
            return self._save_video_ffmpeg(frames, size, fps, output_path)
            
        except Exception as e:
            print(f"åˆ†æ®µè§†é¢‘ä¿å­˜å¤±è´¥: {str(e)}")
            # å¤‡ç”¨æ–¹æ¡ˆï¼šå°è¯•OpenCV
            try:
                fourcc = cv2.VideoWriter_fourcc(*'mp4v')
                out = cv2.VideoWriter(output_path, fourcc, fps, size)
                
                if out.isOpened():
                    for frame in frames:
                        if frame is not None:
                            out.write(frame)
                    out.release()
                    
                    if os.path.exists(output_path) and os.path.getsize(output_path) > 0:
                        return output_path
            except Exception as e2:
                print(f"OpenCVå¤‡ç”¨ä¿å­˜ä¹Ÿå¤±è´¥: {str(e2)}")
            
            raise e
    
    def _save_video_ffmpeg(self, frames, size, fps, output_path):
        """ä½¿ç”¨ffmpegä¿å­˜è§†é¢‘"""
        try:
            # åˆ›å»ºä¸´æ—¶ç›®å½•ä¿å­˜å¸§
            temp_dir = tempfile.mkdtemp()
            frame_files = []
            
            for i, frame in enumerate(frames):
                if frame is None:
                    print(f"è·³è¿‡ç©ºå¸§ {i}")
                    continue
                
                try:
                    # éªŒè¯å¸§æ•°æ®
                    if not isinstance(frame, np.ndarray):
                        print(f"å¸§{i}ä¸æ˜¯numpyæ•°ç»„ï¼Œè·³è¿‡")
                        continue
                    
                    if len(frame.shape) != 3:
                        print(f"å¸§{i}ç»´åº¦é”™è¯¯: {frame.shape}ï¼Œè·³è¿‡")
                        continue
                    
                    # ç¡®ä¿å¸§å°ºå¯¸æ­£ç¡®
                    if frame.shape[:2] != (size[1], size[0]):
                        print(f"è°ƒæ•´å¸§{i}å°ºå¯¸: {frame.shape[:2]} -> {size[1]}x{size[0]}")
                        frame = cv2.resize(frame, size)
                    
                    # ç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®
                    if frame.dtype != np.uint8:
                        frame = frame.astype(np.uint8)
                    
                    # æ³¨æ„ï¼šframeå·²ç»æ˜¯BGRæ ¼å¼ï¼Œéœ€è¦è½¬æ¢ä¸ºRGBç»™PIL
                    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                    pil_image = Image.fromarray(frame_rgb)
                    
                    frame_path = os.path.join(temp_dir, f"frame_{i:06d}.png")
                    pil_image.save(frame_path)
                    frame_files.append(frame_path)
                    
                except Exception as e:
                    print(f"ä¿å­˜å¸§{i}å¤±è´¥: {str(e)}")
                    continue
            
            if not frame_files:
                raise Exception("æ²¡æœ‰æˆåŠŸä¿å­˜ä»»ä½•å¸§")
            
            # ä½¿ç”¨ffmpegåˆæˆè§†é¢‘
            import subprocess
            ffmpeg_cmd = [
                'ffmpeg', '-y',
                '-framerate', str(fps),
                '-i', os.path.join(temp_dir, 'frame_%06d.png'),
                '-c:v', 'libx264',
                '-pix_fmt', 'yuv420p',
                '-crf', '23',
                '-preset', 'fast',
                '-movflags', '+faststart',  # ä¼˜åŒ–ç½‘ç»œæ’­æ”¾
                '-profile:v', 'baseline',   # ä½¿ç”¨åŸºç¡€é…ç½®ï¼Œæé«˜å…¼å®¹æ€§
                '-level', '3.0',            # è®¾ç½®ç¼–ç çº§åˆ«
                output_path
            ]
            
            print(f"æ‰§è¡Œffmpegå‘½ä»¤: {' '.join(ffmpeg_cmd)}")
            result = subprocess.run(ffmpeg_cmd, capture_output=True, text=True)
            
            if result.returncode != 0:
                print(f"ffmpegé”™è¯¯è¾“å‡º: {result.stderr}")
                print(f"ffmpegæ ‡å‡†è¾“å‡º: {result.stdout}")
                raise Exception(f"ffmpegæ‰§è¡Œå¤±è´¥ (è¿”å›ç : {result.returncode}): {result.stderr}")
            
            print(f"ffmpegæ‰§è¡ŒæˆåŠŸï¼Œè¾“å‡ºæ–‡ä»¶: {output_path}")
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            for frame_file in frame_files:
                try:
                    os.remove(frame_file)
                except:
                    pass
            try:
                os.rmdir(temp_dir)
            except:
                pass
            
            return output_path
            
        except Exception as e:
            print(f"ffmpegä¿å­˜å¤±è´¥: {str(e)}")
            # å°è¯•ä½¿ç”¨torchvisionä½œä¸ºæœ€åçš„å¤‡ç”¨æ–¹æ¡ˆ
            try:
                print("å°è¯•ä½¿ç”¨torchvisionä¿å­˜...")
                return self._save_video_torchvision(frames, size, fps, output_path)
            except Exception as e2:
                print(f"torchvisionä¿å­˜ä¹Ÿå¤±è´¥: {str(e2)}")
                raise e
    
    def _merge_segment_videos(self, segment_files, fps):
        """åˆå¹¶åˆ†æ®µè§†é¢‘"""
        try:
            if len(segment_files) == 1:
                return segment_files[0]
            
            # åˆ›å»ºåˆå¹¶æ–‡ä»¶åˆ—è¡¨
            merge_list_path = os.path.join(os.path.dirname(segment_files[0]), 'merge_list.txt')
            with open(merge_list_path, 'w') as f:
                for segment_file in segment_files:
                    f.write(f"file '{segment_file}'\n")
            
            # ä½¿ç”¨ffmpegåˆå¹¶
            import subprocess
            output_path = os.path.join(os.path.dirname(segment_files[0]), 'merged_output.mp4')
            
            ffmpeg_cmd = [
                'ffmpeg', '-y',
                '-f', 'concat',
                '-safe', '0',
                '-i', merge_list_path,
                '-c', 'copy',
                output_path
            ]
            
            result = subprocess.run(ffmpeg_cmd, capture_output=True, text=True)
            
            # æ¸…ç†åˆå¹¶åˆ—è¡¨æ–‡ä»¶
            try:
                os.remove(merge_list_path)
            except:
                pass
            
            if result.returncode != 0:
                raise Exception(f"è§†é¢‘åˆå¹¶å¤±è´¥: {result.stderr}")
            
            return output_path
            
        except Exception as e:
            print(f"è§†é¢‘åˆå¹¶å¤±è´¥: {str(e)}")
            # å¦‚æœåˆå¹¶å¤±è´¥ï¼Œè¿”å›ç¬¬ä¸€ä¸ªåˆ†æ®µæ–‡ä»¶
            return segment_files[0] if segment_files else None

# åˆ›å»ºå…¨å±€æ’å€¼å™¨å®ä¾‹
interpolator = FLAVRInterpolator()

def check_model_files():
    """æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
    status = {}
    for model_name, config in MODEL_CONFIGS.items():
        exists = os.path.exists(config["path"])
        status[model_name] = {
            "exists": exists,
            "path": config["path"],
            "description": config["description"]
        }
    return status

def ensure_compatible_format(video_path):
    """ç¡®ä¿è§†é¢‘æ ¼å¼å…¼å®¹"""
    try:
        # æ£€æŸ¥æ–‡ä»¶æ‰©å±•å
        file_ext = os.path.splitext(video_path)[1].lower()
        
        # å¦‚æœå·²ç»æ˜¯MP4æ ¼å¼ï¼Œç›´æ¥è¿”å›
        if file_ext == '.mp4':
            return video_path
        
        # å°è¯•ä½¿ç”¨OpenCVè¯»å–è§†é¢‘
        cap = cv2.VideoCapture(video_path)
        if cap.isOpened():
            cap.release()
            return video_path  # å¯ä»¥è¯»å–ï¼Œä¸éœ€è¦è½¬æ¢
        
        # å¦‚æœæ— æ³•è¯»å–ï¼Œå°è¯•è½¬æ¢ä¸ºMP4
        print(f"è§†é¢‘æ ¼å¼ {file_ext} å¯èƒ½ä¸å…¼å®¹ï¼Œå°è¯•è½¬æ¢ä¸ºMP4...")
        
        # åˆ›å»ºä¸´æ—¶MP4æ–‡ä»¶
        temp_mp4 = video_path.replace(file_ext, '_converted.mp4')
        
        try:
            import subprocess
            
            # ä½¿ç”¨ffmpegè½¬æ¢
            ffmpeg_cmd = [
                'ffmpeg', '-y',
                '-i', video_path,
                '-c:v', 'libx264',
                '-pix_fmt', 'yuv420p',
                '-crf', '23',
                temp_mp4
            ]
            
            result = subprocess.run(ffmpeg_cmd, capture_output=True, text=True)
            
            if result.returncode == 0 and os.path.exists(temp_mp4):
                print(f"è§†é¢‘è½¬æ¢æˆåŠŸ: {temp_mp4}")
                return temp_mp4
            else:
                print(f"è§†é¢‘è½¬æ¢å¤±è´¥: {result.stderr}")
                return video_path  # è½¬æ¢å¤±è´¥ï¼Œè¿”å›åŸæ–‡ä»¶
                
        except Exception as e:
            print(f"è½¬æ¢è¿‡ç¨‹å‡ºé”™: {str(e)}")
            return video_path  # è½¬æ¢å¤±è´¥ï¼Œè¿”å›åŸæ–‡ä»¶
            
    except Exception as e:
        print(f"æ ¼å¼æ£€æŸ¥å¤±è´¥: {str(e)}")
        return video_path
        
def process_video(video_file, model_name, segment_duration):
    """Gradioå¤„ç†å‡½æ•°"""
    if video_file is None:
        return None, "è¯·ä¸Šä¼ è§†é¢‘æ–‡ä»¶"
    
    if model_name is None:
        return None, "è¯·é€‰æ‹©æ¨¡å‹"
    
    try:
        # åˆ‡æ¢æ¨¡å‹ï¼ˆä¼šè‡ªåŠ¨é‡Šæ”¾å…¶ä»–æ¨¡å‹ï¼‰
        success, msg = interpolator.switch_model(model_name)
        if not success:
            return None, msg
        
        # è·å–è§†é¢‘æ–‡ä»¶è·¯å¾„
        if isinstance(video_file, str):
            video_path = video_file
        elif hasattr(video_file, 'name'):
            video_path = video_file.name
        elif hasattr(video_file, 'file_path'):
            video_path = video_file.file_path
        else:
            return None, "æ— æ³•è·å–è§†é¢‘æ–‡ä»¶è·¯å¾„"
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(video_path):
            return None, f"è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}"
        
        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        file_size = os.path.getsize(video_path)
        if file_size == 0:
            return None, "è§†é¢‘æ–‡ä»¶ä¸ºç©º"
        
        print(f"å¤„ç†è§†é¢‘æ–‡ä»¶: {video_path}, å¤§å°: {file_size / (1024*1024):.2f} MB")
        
        # æ£€æŸ¥è§†é¢‘æ ¼å¼ï¼Œå¦‚æœæ ¼å¼ä¸å…¼å®¹åˆ™è½¬æ¢
        video_path = ensure_compatible_format(video_path)
        
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
        with tempfile.NamedTemporaryFile(suffix='.mp4', delete=False) as tmp_file:
            temp_output = tmp_file.name
        
        print(f"å¼€å§‹å¤„ç†è§†é¢‘ï¼Œåˆ†æ®µæ—¶é•¿: {segment_duration}ç§’")
        
        # æ˜¾ç¤ºå¤„ç†ä¿¡æ¯
        status_msg = f"ğŸ”„ å¼€å§‹å¤„ç†è§†é¢‘...\nğŸ“Š åˆ†æ®µæ—¶é•¿: {segment_duration}ç§’\n"
        
        # å¤„ç†è§†é¢‘
        output_path, size, fps = interpolator.interpolate_video(video_path, segment_duration)
        print("è§†é¢‘å¤„ç†å®Œæˆ")
        
        # å¤åˆ¶åˆ°æœ€ç»ˆè¾“å‡ºä½ç½®
        import shutil
        shutil.copy2(output_path, temp_output)

        print(f"è§†é¢‘å¤„ç†å®Œæˆï¼Œè¾“å‡ºè·¯å¾„: {temp_output}")
        
        factor = interpolator.get_model_factor()
        memory_usage = interpolator.get_memory_usage()
        return output_path, f"å¤„ç†å®Œæˆ! {factor}xæ’å€¼, åˆ†æ®µæ—¶é•¿: {segment_duration}ç§’, è¾“å‡ºFPS: {fps}, å°ºå¯¸: {size[0]}x{size[1]} | {memory_usage}"
        
    except Exception as e:
        traceback.print_exc()
        error_msg = str(e)
        
        # è§†é¢‘æ ¼å¼ç›¸å…³é”™è¯¯
        if "è§†é¢‘å¸§æ•°ä¸è¶³" in error_msg or "è§†é¢‘å¤ªçŸ­" in error_msg:
            return None, f"âŒ {error_msg}\nğŸ’¡ è¯·ä¸Šä¼ æ›´é•¿çš„è§†é¢‘æ–‡ä»¶ï¼ˆè‡³å°‘éœ€è¦4å¸§ï¼‰"
        elif "PyAV is not installed" in error_msg:
            return None, f"âŒ {error_msg}\nğŸ’¡ è¯·å®‰è£…PyAV: pip install av"
        elif "Video not playable" in error_msg or "æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶" in error_msg:
            return None, f"âŒ è§†é¢‘æ ¼å¼ä¸æ”¯æŒ\nğŸ’¡ è¯·å°è¯•ä»¥ä¸‹è§£å†³æ–¹æ¡ˆ:\n1. ç³»ç»Ÿä¼šè‡ªåŠ¨è½¬æ¢æ ¼å¼ï¼Œè¯·ç¨ç­‰\n2. å¦‚æœä»æœ‰é—®é¢˜ï¼Œè¯·æ‰‹åŠ¨è½¬æ¢ä¸ºMP4æ ¼å¼\n3. ä½¿ç”¨H.264ç¼–ç \n4. æ£€æŸ¥è§†é¢‘æ–‡ä»¶æ˜¯å¦æŸå\n5. å°è¯•å…¶ä»–è§†é¢‘æ–‡ä»¶"
        elif "è§†é¢‘è¯»å–å¤±è´¥" in error_msg:
            return None, f"âŒ {error_msg}\nğŸ’¡ è¯·æ£€æŸ¥:\n1. è§†é¢‘æ–‡ä»¶æ˜¯å¦å®Œæ•´\n2. æ ¼å¼æ˜¯å¦æ”¯æŒ(MP4, AVI, MOVç­‰)\n3. æ–‡ä»¶æ˜¯å¦æŸå"
        elif "è§†é¢‘ä¿å­˜å¤±è´¥" in error_msg:
            return None, f"âŒ {error_msg}\nğŸ’¡ è¯·æ£€æŸ¥:\n1. ç£ç›˜ç©ºé—´æ˜¯å¦å……è¶³\n2. æ˜¯å¦æœ‰å†™å…¥æƒé™\n3. å°è¯•é‡æ–°å¤„ç†"
        else:
            return None, f"âŒ å¤„ç†å¤±è´¥: {error_msg}"

def clear_model_cache():
    """æ¸…ç†æ¨¡å‹ç¼“å­˜"""
    interpolator.clear_models()
    return interpolator.get_memory_usage(), "æ¨¡å‹ç¼“å­˜å·²æ¸…ç†"

def update_model_status():
    """æ›´æ–°æ¨¡å‹çŠ¶æ€æ˜¾ç¤º"""
    model_status = check_model_files()
    model_choices = []
    model_descriptions = []
    
    for model_name, config in MODEL_CONFIGS.items():
        status = model_status[model_name]
        if status["exists"]:
            model_choices.append(model_name)
            model_descriptions.append(f"âœ… {config['description']}")
        else:
            model_choices.append(model_name)
            model_descriptions.append(f"âŒ {config['description']} (æœªä¸‹è½½)")
    
    loaded_models_info = interpolator.get_loaded_models_info()
    status_text = "### æ¨¡å‹çŠ¶æ€:\n" + "\n".join([f"- {desc}" for desc in model_descriptions]) + \
                  "\n\n### å·²åŠ è½½æ¨¡å‹:\n" + "\n".join([f"- {info}" for info in loaded_models_info])
    
    return model_choices, status_text, interpolator.get_memory_usage()

def check_video_format(video_file):
    """æ£€æŸ¥è§†é¢‘æ ¼å¼"""
    if video_file is None:
        return "è¯·ä¸Šä¼ è§†é¢‘æ–‡ä»¶"
    
    try:
        # è·å–è§†é¢‘æ–‡ä»¶è·¯å¾„
        if isinstance(video_file, str):
            video_path = video_file
        elif hasattr(video_file, 'name'):
            video_path = video_file.name
        elif hasattr(video_file, 'file_path'):
            video_path = video_file.file_path
        else:
            return "æ— æ³•è·å–è§†é¢‘æ–‡ä»¶è·¯å¾„"
        
        # æ£€æŸ¥æ–‡ä»¶æ‰©å±•å
        file_ext = os.path.splitext(video_path)[1].lower()
        supported_formats = ['.mp4', '.avi', '.mov', '.mkv', '.wmv', '.flv', '.webm']
        
        if file_ext not in supported_formats:
            return f"âš ï¸ ä¸æ”¯æŒçš„æ ¼å¼: {file_ext}\nğŸ’¡ å»ºè®®è½¬æ¢ä¸ºMP4æ ¼å¼"
        
        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        file_size = os.path.getsize(video_path)
        if file_size == 0:
            return "âŒ è§†é¢‘æ–‡ä»¶ä¸ºç©º"
        
        # å°è¯•è¯»å–è§†é¢‘ä¿¡æ¯
        try:
            cap = cv2.VideoCapture(video_path)
            if not cap.isOpened():
                return "âŒ æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶ï¼Œå¯èƒ½æ ¼å¼ä¸æ”¯æŒæˆ–æ–‡ä»¶æŸå"
            
            fps = cap.get(cv2.CAP_PROP_FPS)
            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            
            cap.release()
            
            if frame_count < 4:
                return f"âŒ è§†é¢‘å¸§æ•°ä¸è¶³: {frame_count}å¸§ (éœ€è¦è‡³å°‘4å¸§)"
            
            return f"âœ… è§†é¢‘æ ¼å¼æ­£å¸¸\nğŸ“Š ä¿¡æ¯: {width}x{height}, {fps:.1f}FPS, {frame_count}å¸§, {file_size/(1024*1024):.1f}MB"
            
        except Exception as e:
            return f"âš ï¸ è§†é¢‘ä¿¡æ¯è¯»å–å¤±è´¥: {str(e)}"
            
    except Exception as e:
        return f"âŒ è§†é¢‘æ£€æŸ¥å¤±è´¥: {str(e)}"

def create_interface():
    """åˆ›å»ºGradioç•Œé¢"""
    with gr.Blocks(title="FLAVR è§†é¢‘è¡¥å¸§å·¥å…·", theme=gr.themes.Soft()) as demo:
        gr.Markdown("""
        # ğŸ¬ FLAVR è§†é¢‘è¡¥å¸§å·¥å…·
        
        FLAVRæ˜¯ä¸€ç§å¿«é€Ÿã€æ— æµçš„å¸§æ’å€¼æ–¹æ³•ï¼Œèƒ½å¤Ÿè¿›è¡Œå•æ¬¡å¤šå¸§é¢„æµ‹ã€‚ä¸Šä¼ æ‚¨çš„è§†é¢‘ï¼Œé€‰æ‹©æ’å€¼æ¨¡å‹ï¼Œå³å¯ç”Ÿæˆé«˜å¸§ç‡è§†é¢‘ã€‚
        
        ### ä½¿ç”¨è¯´æ˜:
        1. ä¸Šä¼ è§†é¢‘æ–‡ä»¶ (æ”¯æŒmp4, aviç­‰æ ¼å¼ï¼Œè‡³å°‘éœ€è¦4å¸§)
        2. é€‰æ‹©é¢„è®­ç»ƒæ¨¡å‹ (æ’å€¼å€æ•°è‡ªåŠ¨ç¡®å®š)
        3. ç‚¹å‡»å¼€å§‹å¤„ç†
        
        ### æ¨¡å‹è¯´æ˜:
        - **2xæ’å€¼**: é€‚ç”¨äº30FPSâ†’60FPS
        - **4xæ’å€¼**: é€‚ç”¨äº30FPSâ†’120FPS  
        - **8xæ’å€¼**: é€‚ç”¨äº30FPSâ†’240FPS
        
        ### ğŸš€ æ–°åŠŸèƒ½:
        - **è§†é¢‘é¢„è§ˆ**: æ”¯æŒè§†é¢‘ä¸Šä¼ å‰é¢„è§ˆ
        - **æ™ºèƒ½å†…å­˜ç®¡ç†**: æŒ‰éœ€åŠ è½½æ¨¡å‹ï¼Œåˆ‡æ¢æ—¶è‡ªåŠ¨é‡Šæ”¾å†…å­˜
        - **å®æ—¶çŠ¶æ€ç›‘æ§**: æ˜¾ç¤ºå†…å­˜ä½¿ç”¨å’Œæ¨¡å‹çŠ¶æ€
        - **åˆ†æ®µå­˜å‚¨å¤„ç†**: çœŸæ­£çš„åˆ†æ®µå¤„ç†ï¼Œæ¯æ®µå¤„ç†å®Œæˆåç«‹å³ä¿å­˜åˆ°æœ¬åœ°ï¼Œå½»åº•è§£å†³å†…å­˜é—®é¢˜
        
        ### âš ï¸ å†…å­˜ä½¿ç”¨æç¤º:
        - å¤„ç†å¤§åˆ†è¾¨ç‡è§†é¢‘æ—¶å¯èƒ½éœ€è¦å¤§é‡å†…å­˜
        - ç³»ç»Ÿä¼šè‡ªåŠ¨æ˜¾ç¤ºé¢„ä¼°å†…å­˜éœ€æ±‚
        - å¦‚é‡å†…å­˜ä¸è¶³ï¼Œè¯·ä½¿ç”¨è¾ƒå°çš„è§†é¢‘æ–‡ä»¶æˆ–é™ä½åˆ†è¾¨ç‡
        - **åˆ†æ®µå¤„ç†**: å¯é€šè¿‡è°ƒæ•´åˆ†æ®µæ—¶é•¿æ¥æ§åˆ¶å†…å­˜ä½¿ç”¨
        
        ### ğŸ“¹ æ”¯æŒçš„è§†é¢‘æ ¼å¼:
        - **æ¨èæ ¼å¼**: MP4 (H.264ç¼–ç )
        - **å…¶ä»–æ ¼å¼**: AVI, MOV, MKV, WMV
        - **è‡ªåŠ¨è½¬æ¢**: ç³»ç»Ÿä¼šè‡ªåŠ¨è½¬æ¢ä¸å…¼å®¹çš„æ ¼å¼ä¸ºMP4
        - **æ³¨æ„äº‹é¡¹**: ç¡®ä¿è§†é¢‘æ–‡ä»¶å®Œæ•´ä¸”æœªæŸå
        """)
        
        # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶çŠ¶æ€
        model_status = check_model_files()
        
        with gr.Row():
            with gr.Column(scale=1):
                # è¾“å…¥éƒ¨åˆ†
                video_input = gr.Video(
                    label="ä¸Šä¼ è§†é¢‘æ–‡ä»¶"
                )
                
                # è§†é¢‘æ ¼å¼æ£€æŸ¥
                video_check_btn = gr.Button(
                    "æ£€æŸ¥è§†é¢‘æ ¼å¼",
                    variant="secondary",
                    size="sm"
                )
                
                video_status = gr.Textbox(
                    label="è§†é¢‘çŠ¶æ€",
                    interactive=False,
                    lines=3
                )
                
                # æ¨¡å‹é€‰æ‹©
                model_choices = []
                model_descriptions = []
                for model_name, config in MODEL_CONFIGS.items():
                    status = model_status[model_name]
                    if status["exists"]:
                        model_choices.append(model_name)
                        model_descriptions.append(f"âœ… {config['description']}")
                    else:
                        model_choices.append(model_name)
                        model_descriptions.append(f"âŒ {config['description']} (æœªä¸‹è½½)")
                
                model_input = gr.Dropdown(
                    choices=model_choices,
                    label="é€‰æ‹©æ¨¡å‹",
                    info="æ’å€¼å€æ•°ç”±æ¨¡å‹è‡ªåŠ¨ç¡®å®š",
                    value=model_choices[0] if model_choices else None
                )
                
                # åˆ†æ®µæ—¶é•¿æ§åˆ¶
                segment_duration = gr.Slider(
                    minimum=1,
                    maximum=60,
                    value=5,
                    step=1,
                    label="åˆ†æ®µæ—¶é•¿ (ç§’)",
                    info="æ§åˆ¶åˆ†æ®µå¤„ç†çš„æ®µé•¿åº¦ï¼Œæ¯æ®µå¤„ç†å®Œæˆåç«‹å³ä¿å­˜ï¼Œæ¨è5-10ç§’"
                )
                
                # æ˜¾ç¤ºæ¨¡å‹çŠ¶æ€
                model_status_text = gr.Markdown(
                    value="### æ¨¡å‹çŠ¶æ€:\n" + "\n".join([f"- {desc}" for desc in model_descriptions])
                )
                
                process_btn = gr.Button(
                    "å¼€å§‹å¤„ç†",
                    variant="primary",
                    size="lg"
                )
                
                # å†…å­˜ç®¡ç†
                memory_info = gr.Textbox(
                    label="å†…å­˜ä½¿ç”¨æƒ…å†µ",
                    value=interpolator.get_memory_usage(),
                    interactive=False
                )
                
                clear_btn = gr.Button(
                    "æ¸…ç†æ¨¡å‹ç¼“å­˜",
                    variant="secondary",
                    size="sm"
                )
                
                refresh_btn = gr.Button(
                    "åˆ·æ–°çŠ¶æ€",
                    variant="secondary",
                    size="sm"
                )
            
            with gr.Column(scale=1):
                # è¾“å‡ºéƒ¨åˆ†
                video_output = gr.Video(
                    label="å¤„ç†ç»“æœ"
                )
                
                status_output = gr.Textbox(
                    label="å¤„ç†çŠ¶æ€",
                    interactive=False,
                    lines=3
                )
        
        # å¤„ç†é€»è¾‘
        process_btn.click(
            fn=process_video,
            inputs=[video_input, model_input, segment_duration],
            outputs=[video_output, status_output]
        )
        
        # æ¸…ç†ç¼“å­˜
        clear_btn.click(
            fn=clear_model_cache,
            inputs=[],
            outputs=[memory_info, status_output]
        )
        
        # åˆ·æ–°çŠ¶æ€
        refresh_btn.click(
            fn=update_model_status,
            inputs=[],
            outputs=[model_input, model_status_text, memory_info]
        )
        
        # è§†é¢‘æ ¼å¼æ£€æŸ¥
        video_check_btn.click(
            fn=check_video_format,
            inputs=[video_input],
            outputs=[video_status]
        )
        
        # æ·»åŠ æ¨¡å‹ä¸‹è½½è¯´æ˜
        gr.Markdown("""
        ### ğŸ“¥ æ¨¡å‹ä¸‹è½½è¯´æ˜:
        
        è¯·å°†ä¸‹è½½çš„æ¨¡å‹æ–‡ä»¶é‡å‘½åå¹¶æ”¾ç½®åˆ°ä»¥ä¸‹è·¯å¾„:
        
        ```
        models/
        â”œâ”€â”€ flavr_2x.pth  # 2xæ’å€¼æ¨¡å‹
        â”œâ”€â”€ flavr_4x.pth  # 4xæ’å€¼æ¨¡å‹
        â””â”€â”€ flavr_8x.pth  # 8xæ’å€¼æ¨¡å‹
        ```
        
        ### ğŸ”— æ¨¡å‹ä¸‹è½½é“¾æ¥:
        - [2xæ’å€¼æ¨¡å‹](https://drive.google.com/file/d/1IZe-39ZuXy3OheGJC-fT3shZocGYuNdH/view?usp=sharing) â†’ é‡å‘½åä¸º `flavr_2x.pth`
        - [4xæ’å€¼æ¨¡å‹](https://drive.google.com/file/d/1GARJK0Ti1gLH_O0spxAEqzbMwUKqE37S/view?usp=sharing) â†’ é‡å‘½åä¸º `flavr_4x.pth`
        - [8xæ’å€¼æ¨¡å‹](https://drive.google.com/file/d/1xoZqWJdIOjSaE2DtH4ifXKlRwFySm5Gq/view?usp=sharing) â†’ é‡å‘½åä¸º `flavr_8x.pth`
        
        ### âš¡ æ€§èƒ½ä¼˜åŒ–:
        - **æ™ºèƒ½åˆ‡æ¢**: åˆ‡æ¢æ¨¡å‹æ—¶è‡ªåŠ¨é‡Šæ”¾å…¶ä»–æ¨¡å‹ï¼ŒèŠ‚çœå†…å­˜
        - **å†…å­˜ç®¡ç†**: å¯æ‰‹åŠ¨æ¸…ç†ä¸éœ€è¦çš„æ¨¡å‹ç¼“å­˜
        - **GPUåŠ é€Ÿ**: æ”¯æŒCUDAåŠ é€Ÿï¼Œå¤§å¹…æå‡å¤„ç†é€Ÿåº¦
        - **å®æ—¶çŠ¶æ€**: æ˜¾ç¤ºå½“å‰å†…å­˜ä½¿ç”¨å’Œæ¨¡å‹åŠ è½½çŠ¶æ€
        
        ### æ³¨æ„äº‹é¡¹:
        - å¤„ç†æ—¶é—´å–å†³äºè§†é¢‘é•¿åº¦å’Œåˆ†è¾¨ç‡
        - å»ºè®®ä½¿ç”¨GPUåŠ é€Ÿå¤„ç†
        - è¾“å‡ºè§†é¢‘å°†è‡ªåŠ¨è°ƒæ•´åˆ°8çš„å€æ•°åˆ†è¾¨ç‡
        - æ”¯æŒå¸¸è§è§†é¢‘æ ¼å¼: MP4, AVI, MOVç­‰
        - åˆ‡æ¢æ¨¡å‹æ—¶ä¼šè‡ªåŠ¨é‡Šæ”¾å…¶ä»–æ¨¡å‹å†…å­˜ï¼Œæé«˜ç³»ç»Ÿæ€§èƒ½
        - **è§†é¢‘è¦æ±‚**: è‡³å°‘éœ€è¦4å¸§æ‰èƒ½è¿›è¡Œæ’å€¼å¤„ç†
        - **å¸§ç‡è¯´æ˜**: è¾“å‡ºå¸§ç‡ = åŸå§‹å¸§ç‡ Ã— æ’å€¼å€æ•° (å¦‚30FPSè¾“å…¥ï¼Œ2xæ’å€¼è¾“å‡º60FPS)
        - **å†…å­˜è¦æ±‚**: å¤§åˆ†è¾¨ç‡è§†é¢‘å¤„ç†éœ€è¦å……è¶³å†…å­˜ï¼Œç³»ç»Ÿä¼šæ˜¾ç¤ºé¢„ä¼°å†…å­˜éœ€æ±‚
        - **å†…å­˜ä¸è¶³æ—¶**: è¯·ä½¿ç”¨è¾ƒå°çš„è§†é¢‘æ–‡ä»¶æˆ–é™ä½è§†é¢‘åˆ†è¾¨ç‡
        - **åˆ†æ®µå­˜å‚¨å¤„ç†**: é•¿è§†é¢‘ä¼šåˆ†æ®µå¤„ç†å¹¶ç«‹å³ä¿å­˜åˆ°æœ¬åœ°ï¼Œå½»åº•è§£å†³å†…å­˜é—®é¢˜
        - **åˆ†æ®µæ—¶é•¿å»ºè®®**: å†…å­˜è¾ƒå°å»ºè®®3-5ç§’ï¼Œå†…å­˜å……è¶³å¯è®¾ç½®8-15ç§’
        - **å†…å­˜ä¼˜åŒ–**: æ¯æ®µå¤„ç†å®Œæˆåç«‹å³ä¿å­˜å¹¶æ¸…ç†å†…å­˜ï¼Œæ”¯æŒå¤„ç†è¾ƒé•¿çš„è§†é¢‘
        """)
    
    return demo

if __name__ == "__main__":
    # åˆ›å»ºå¹¶å¯åŠ¨ç•Œé¢
    demo = create_interface()
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=True,
        debug=True
    ) 